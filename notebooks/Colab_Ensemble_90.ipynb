{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ðŸŽ¯ Chest X-Ray Ensemble - 90%+ Target\n\n**Baseline**: 82.3% | **Target**: 90%+\n\n## Optimizations:\n- torch.compile (20-40% speedup)\n- 384px resolution option\n- GPU cache + aggressive augmentation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Config"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========== USER CONFIG ==========\nUSE_384 = True                  # True=Higher accuracy (384px), False=Faster (224px)\nFAST_TEST = False               # True=5 min test, False=Full training\n\n# ========== Setup ==========\nimport torch, os, shutil\n\nif not torch.cuda.is_available():\n    raise Exception(\"GPU required\")\n\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nprint(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nprint(f\"Mode: {'384px (accuracy)' if USE_384 else '224px (speed)'}\")\nprint(f\"Test: {'Fast (5min)' if FAST_TEST else 'Full training'}\")\n\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\n# Clone repo\n%cd /content\nif os.path.exists('nycu-CSIC30014-LAB3'):\n    shutil.rmtree('nycu-CSIC30014-LAB3')\n!git clone -q https://github.com/thc1006/nycu-CSIC30014-LAB3.git\n%cd nycu-CSIC30014-LAB3\n\n# Install\n!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n!pip install -q numpy pandas scikit-learn matplotlib tqdm pyyaml opencv-python kaggle scipy\n\n# Kaggle API\nfrom google.colab import files as colab_files\nfrom pathlib import Path\n\nuploaded = colab_files.upload()\nif 'kaggle.json' not in uploaded:\n    raise Exception(\"Upload kaggle.json\")\n\nkaggle_dir = Path.home() / '.kaggle'\nkaggle_dir.mkdir(exist_ok=True)\nkaggle_json = kaggle_dir / 'kaggle.json'\nwith open(kaggle_json, 'wb') as f:\n    f.write(uploaded['kaggle.json'])\nos.chmod(kaggle_json, 0o600)\n\nprint(\"âœ… Setup complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Download\n",
    "print(\"\\n[1/2] Downloading competition data...\")\n",
    "result = subprocess.run(\n",
    "    ['kaggle', 'competitions', 'download', '-c', 'cxr-multi-label-classification'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    if '403' in result.stderr:\n",
    "        print(\"âŒ You need to join the competition first!\")\n",
    "        print(\"Visit: https://www.kaggle.com/competitions/cxr-multi-label-classification\")\n",
    "        raise Exception(\"Join competition\")\n",
    "    else:\n",
    "        raise Exception(f\"Download failed: {result.stderr}\")\n",
    "\n",
    "# Extract\n",
    "for zip_file in [f for f in os.listdir('.') if f.endswith('.zip')]:\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        for file in tqdm(zf.namelist(), desc=f\"Extracting {zip_file}\", leave=False):\n",
    "            zf.extract(file, '.')\n",
    "    os.remove(zip_file)\n",
    "\n",
    "print(\"âœ… Data downloaded & extracted\")\n",
    "\n",
    "# Organize\n",
    "print(\"\\n[2/2] Organizing images...\")\n",
    "\n",
    "# Collect all images\n",
    "all_images = {}\n",
    "for search_dir in ['.', 'train_images', 'val_images', 'test_images']:\n",
    "    if os.path.exists(search_dir):\n",
    "        for fname in os.listdir(search_dir):\n",
    "            if fname.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                if fname not in all_images:\n",
    "                    all_images[fname] = os.path.join(search_dir, fname)\n",
    "\n",
    "# Move CSVs to data/\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for csv in ['train_data.csv', 'val_data.csv', 'test_data.csv']:\n",
    "    if os.path.exists(csv) and not os.path.exists(f'data/{csv}'):\n",
    "        shutil.move(csv, f'data/{csv}')\n",
    "\n",
    "# Organize by split\n",
    "splits = {\n",
    "    'train': ('data/train_data.csv', 'train_images'),\n",
    "    'val': ('data/val_data.csv', 'val_images'),\n",
    "    'test': ('data/test_data.csv', 'test_images')\n",
    "}\n",
    "\n",
    "for split_name, (csv_path, target_dir) in splits.items():\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        needed_files = set(df['new_filename'].values)\n",
    "        \n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        for fname in tqdm(needed_files, desc=f\"{split_name.upper()}\", leave=False):\n",
    "            target_path = os.path.join(target_dir, fname)\n",
    "            if not os.path.exists(target_path) and fname in all_images:\n",
    "                source_path = all_images[fname]\n",
    "                if os.path.abspath(source_path) != os.path.abspath(target_path):\n",
    "                    try:\n",
    "                        shutil.move(source_path, target_path)\n",
    "                    except FileNotFoundError:\n",
    "                        pass\n",
    "\n",
    "# Verify\n",
    "print(\"\\nVerification:\")\n",
    "for split_name, (csv_path, target_dir) in splits.items():\n",
    "    if os.path.exists(target_dir):\n",
    "        count = len([f for f in os.listdir(target_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"  {target_dir}: {count} images\")\n",
    "\n",
    "print(\"\\nâœ… Data organized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train 1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if FAST_TEST:\n    !python -m src.train_v2 --config configs/fast_test.yaml\nelse:\n    suffix = \"_384\" if USE_384 else \"\"\n    !python -m src.train_v2 --config configs/colab_convnext_tiny{suffix}.yaml\nprint(\"âœ… Model 1 done\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not FAST_TEST:\n    suffix = \"_384\" if USE_384 else \"\"\n    !python -m src.train_v2 --config configs/colab_efficientnetv2_s{suffix}.yaml\n    print(\"âœ… Model 2 done\")\nelse:\n    print(\"â­ï¸ Skipped in FAST_TEST mode\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train 3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not FAST_TEST:\n    suffix = \"_384\" if USE_384 else \"\"\n    !python -m src.train_v2 --config configs/colab_regnetx_3.2gf{suffix}.yaml\n    print(\"âœ… Model 3 done\")\nelse:\n    print(\"â­ï¸ Skipped in FAST_TEST mode\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## TTA Prediction"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\n\nif FAST_TEST:\n    models = [('fast_test', 'configs/fast_test.yaml', 'outputs/fast_test/best.pt')]\nelse:\n    suffix = \"_384\" if USE_384 else \"\"\n    models = [\n        ('convnext', f'configs/colab_convnext_tiny{suffix}.yaml', f'outputs/colab_convnext_tiny{suffix}/best.pt'),\n        ('efficientnet', f'configs/colab_efficientnetv2_s{suffix}.yaml', f'outputs/colab_efficientnetv2_s{suffix}/best.pt'),\n        ('regnet', f'configs/colab_regnetx_3.2gf{suffix}.yaml', f'outputs/colab_regnetx_3.2gf{suffix}/best.pt')\n    ]\n\nfor name, config, ckpt in models:\n    !python -m src.tta_predict --config {config} --ckpt {ckpt}\n    \n    with open(config, 'r') as f:\n        cfg = yaml.safe_load(f)\n    submission_path = cfg['out']['submission_path']\n    \n    if os.path.exists('submission_tta.csv'):\n        shutil.move('submission_tta.csv', submission_path)\n        \nprint(\"âœ… TTA done\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Ensemble"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd, numpy as np\nfrom scipy.optimize import minimize\n\nif FAST_TEST:\n    print(\"â­ï¸ Skip ensemble in FAST_TEST mode\")\nelse:\n    suffix = \"_384\" if USE_384 else \"\"\n    pred1 = pd.read_csv(f'data/submission_convnext_tiny{suffix}.csv')\n    pred2 = pd.read_csv(f'data/submission_efficientnetv2_s{suffix}.csv')\n    pred3 = pd.read_csv(f'data/submission_regnetx_3.2gf{suffix}.csv')\n    \n    prob_cols = ['normal', 'bacteria', 'virus', 'COVID-19']\n    \n    # Temperature scaling\n    def temp_scale(logits, T):\n        return logits / T\n    \n    def find_T(probs):\n        avg_max = np.max(probs, axis=1).mean()\n        return 1.5 if avg_max > 0.9 else (0.8 if avg_max < 0.7 else 1.0)\n    \n    T1 = find_T(pred1[prob_cols].values)\n    T2 = find_T(pred2[prob_cols].values)\n    T3 = find_T(pred3[prob_cols].values)\n    \n    scaled1 = np.exp(temp_scale(np.log(pred1[prob_cols].values + 1e-10), T1))\n    scaled1 /= scaled1.sum(axis=1, keepdims=True)\n    scaled2 = np.exp(temp_scale(np.log(pred2[prob_cols].values + 1e-10), T2))\n    scaled2 /= scaled2.sum(axis=1, keepdims=True)\n    scaled3 = np.exp(temp_scale(np.log(pred3[prob_cols].values + 1e-10), T3))\n    scaled3 /= scaled3.sum(axis=1, keepdims=True)\n    \n    # Weighted ensemble\n    weights = np.array([0.87, 0.88, 0.86])\n    weights /= weights.sum()\n    \n    ensemble = pred1.copy()\n    ensemble[prob_cols] = weights[0] * scaled1 + weights[1] * scaled2 + weights[2] * scaled3\n    preds = ensemble[prob_cols].values.argmax(axis=1)\n    ensemble[prob_cols] = np.eye(4)[preds]\n    ensemble.to_csv('submission_ensemble.csv', index=False)\n    \n    print(\"âœ… Ensemble done\")\n    for col in prob_cols:\n        count = ensemble[col].sum()\n        print(f\"  {col}: {int(count)} ({count/len(ensemble)*100:.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Download"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files as colab_files\n\nif FAST_TEST:\n    files = ['data/submission_fast_test.csv']\nelse:\n    files = ['submission_ensemble.csv']\n\nfor f in files:\n    if os.path.exists(f):\n        colab_files.download(f)\n        print(f\"âœ… {f}\")\n        \nprint(\"\\nðŸŽ¯ Expected: 88-92% (384px) or 85-88% (224px)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}