{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Chest X-Ray Multi-CNN Ensemble Strategy - Target 90%\n",
    "\n",
    "**Current Best**: ResNet18 = 82.322%  \n",
    "**Target**: 90%+ via ensemble\n",
    "\n",
    "## üìã Strategy:\n",
    "\n",
    "**Why abandon ViT?**\n",
    "- ViT (80.3% ‚Üí 82.6%) failed to beat ResNet18 (82.3%)\n",
    "- 86M parameters too large for 3780 training samples\n",
    "- Medical imaging favors **local features** (CNN) over global (Transformer)\n",
    "\n",
    "**New approach: 4-Model CNN Ensemble**\n",
    "1. ResNet18 (11.7M) - 82.3% ‚úÖ proven\n",
    "2. ResNet50 (25.6M) - deeper representation\n",
    "3. DenseNet121 (8.1M) - excellent for medical imaging\n",
    "4. MobileNetV2 (3.4M) - lightweight, low overfitting\n",
    "\n",
    "**Expected**: Individual 83-86% ‚Üí Ensemble **88-90%** üéØ\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Time Required:\n",
    "\n",
    "- Setup: 5-10 minutes\n",
    "- Training (3 models): 90-120 minutes (A100)\n",
    "- Ensemble: 5 minutes\n",
    "- **Total**: ~2-2.5 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Before You Start:\n",
    "\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí **A100 GPU**\n",
    "2. Get Kaggle API key from https://www.kaggle.com/settings\n",
    "3. Join competition: https://www.kaggle.com/competitions/cxr-multi-label-classification\n",
    "4. Click \"Run all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup (GPU + Clone + Install)\n",
    "\n",
    "Combines GPU verification, repository cloning, and dependency installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# ========== GPU Verification ==========\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"\\n‚ùå NO GPU! Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    raise Exception(\"GPU required\")\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "print(f\"\\n‚úÖ GPU: {gpu_name}\")\n",
    "print(f\"‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ========== Clone Repository ==========\n",
    "print(\"\\n[1/3] Cloning repository...\")\n",
    "%cd /content\n",
    "if os.path.exists('nycu-CSIC30014-LAB3'):\n",
    "    shutil.rmtree('nycu-CSIC30014-LAB3')\n",
    "\n",
    "!git clone -q https://github.com/thc1006/nycu-CSIC30014-LAB3.git\n",
    "%cd nycu-CSIC30014-LAB3\n",
    "print(\"‚úÖ Repository cloned\")\n",
    "\n",
    "# ========== Install Dependencies ==========\n",
    "print(\"\\n[2/3] Installing dependencies...\")\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q numpy pandas scikit-learn matplotlib tqdm pyyaml opencv-python kaggle\n",
    "print(\"‚úÖ Dependencies installed\")\n",
    "\n",
    "# ========== Kaggle Setup ==========\n",
    "print(\"\\n[3/3] Setting up Kaggle API...\")\n",
    "from google.colab import files as colab_files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Please upload your kaggle.json:\")\n",
    "uploaded = colab_files.upload()\n",
    "\n",
    "if 'kaggle.json' not in uploaded:\n",
    "    raise Exception(\"Please upload kaggle.json\")\n",
    "\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "kaggle_json = kaggle_dir / 'kaggle.json'\n",
    "with open(kaggle_json, 'wb') as f:\n",
    "    f.write(uploaded['kaggle.json'])\n",
    "os.chmod(kaggle_json, 0o600)\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download & Organize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 2: DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Download\n",
    "print(\"\\n[1/2] Downloading competition data...\")\n",
    "result = subprocess.run(\n",
    "    ['kaggle', 'competitions', 'download', '-c', 'cxr-multi-label-classification'],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    if '403' in result.stderr:\n",
    "        print(\"‚ùå You need to join the competition first!\")\n",
    "        print(\"Visit: https://www.kaggle.com/competitions/cxr-multi-label-classification\")\n",
    "        raise Exception(\"Join competition\")\n",
    "    else:\n",
    "        raise Exception(f\"Download failed: {result.stderr}\")\n",
    "\n",
    "# Extract\n",
    "for zip_file in [f for f in os.listdir('.') if f.endswith('.zip')]:\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zf:\n",
    "        for file in tqdm(zf.namelist(), desc=f\"Extracting {zip_file}\", leave=False):\n",
    "            zf.extract(file, '.')\n",
    "    os.remove(zip_file)\n",
    "\n",
    "print(\"‚úÖ Data downloaded & extracted\")\n",
    "\n",
    "# Organize\n",
    "print(\"\\n[2/2] Organizing images...\")\n",
    "\n",
    "# Collect all images\n",
    "all_images = {}\n",
    "for search_dir in ['.', 'train_images', 'val_images', 'test_images']:\n",
    "    if os.path.exists(search_dir):\n",
    "        for fname in os.listdir(search_dir):\n",
    "            if fname.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                if fname not in all_images:\n",
    "                    all_images[fname] = os.path.join(search_dir, fname)\n",
    "\n",
    "# Move CSVs to data/\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for csv in ['train_data.csv', 'val_data.csv', 'test_data.csv']:\n",
    "    if os.path.exists(csv) and not os.path.exists(f'data/{csv}'):\n",
    "        shutil.move(csv, f'data/{csv}')\n",
    "\n",
    "# Organize by split\n",
    "splits = {\n",
    "    'train': ('data/train_data.csv', 'train_images'),\n",
    "    'val': ('data/val_data.csv', 'val_images'),\n",
    "    'test': ('data/test_data.csv', 'test_images')\n",
    "}\n",
    "\n",
    "for split_name, (csv_path, target_dir) in splits.items():\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        needed_files = set(df['new_filename'].values)\n",
    "        \n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        for fname in tqdm(needed_files, desc=f\"{split_name.upper()}\", leave=False):\n",
    "            target_path = os.path.join(target_dir, fname)\n",
    "            if not os.path.exists(target_path) and fname in all_images:\n",
    "                source_path = all_images[fname]\n",
    "                if os.path.abspath(source_path) != os.path.abspath(target_path):\n",
    "                    try:\n",
    "                        shutil.move(source_path, target_path)\n",
    "                    except FileNotFoundError:\n",
    "                        pass\n",
    "\n",
    "# Verify\n",
    "print(\"\\nVerification:\")\n",
    "for split_name, (csv_path, target_dir) in splits.items():\n",
    "    if os.path.exists(target_dir):\n",
    "        count = len([f for f in os.listdir(target_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        print(f\"  {target_dir}: {count} images\")\n",
    "\n",
    "print(\"\\n‚úÖ Data organized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train Model 1 - ResNet50 (~40 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: TRAIN RESNET50\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModel: ResNet50 (25.6M params)\")\n",
    "print(\"Expected: 84-86% | Time: ~40 min (A100)\\n\")\n",
    "\n",
    "# Auto-adjust for T4\n",
    "if 'T4' in torch.cuda.get_device_name(0):\n",
    "    !sed -i 's/batch_size: 24/batch_size: 16/g' configs/colab_resnet50.yaml\n",
    "    print(\"[INFO] T4 detected: batch_size 24 ‚Üí 16\")\n",
    "\n",
    "!python -m src.train_v2 --config configs/colab_resnet50.yaml\n",
    "\n",
    "print(\"\\n‚úÖ ResNet50 training complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train Model 2 - DenseNet121 (~40 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 4: TRAIN DENSENET121\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModel: DenseNet121 (8.1M params)\")\n",
    "print(\"Expected: 84-86% | Time: ~40 min (A100)\\n\")\n",
    "\n",
    "!python -m src.train_v2 --config configs/colab_densenet121.yaml\n",
    "\n",
    "print(\"\\n‚úÖ DenseNet121 training complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Model 3 - MobileNetV2 (~35 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"STEP 5: TRAIN MOBILENETV2\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nModel: MobileNetV2 (3.4M params - lightweight!)\")\n",
    "print(\"Expected: 83-85% | Time: ~35 min (A100)\\n\")\n",
    "\n",
    "!python -m src.train_v2 --config configs/colab_mobilenetv2.yaml\n",
    "\n",
    "print(\"\\n‚úÖ MobileNetV2 training complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate TTA Predictions (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 60)\nprint(\"STEP 6: GENERATE TTA PREDICTIONS\")\nprint(\"=\" * 60)\n\nmodels = [\n    ('ResNet50', 'configs/colab_resnet50.yaml', 'outputs/colab_resnet50/best.pt'),\n    ('DenseNet121', 'configs/colab_densenet121.yaml', 'outputs/colab_densenet121/best.pt'),\n    ('MobileNetV2', 'configs/colab_mobilenetv2.yaml', 'outputs/colab_mobilenetv2/best.pt')\n]\n\nfor model_name, config, ckpt in models:\n    print(f\"\\nGenerating TTA predictions for {model_name}...\")\n    !python -m src.tta_predict --config {config} --ckpt {ckpt}\n    \n    # Rename output based on config's submission_path\n    import yaml\n    with open(config, 'r') as f:\n        cfg = yaml.safe_load(f)\n    \n    submission_path = cfg['out']['submission_path']\n    \n    if os.path.exists('submission_tta.csv'):\n        shutil.move('submission_tta.csv', submission_path)\n        print(f\"‚úÖ Saved to {submission_path}\")\n\nprint(\"\\n‚úÖ All TTA predictions generated\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create 4-Model Ensemble üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\n\nprint(\"=\" * 60)\nprint(\"STEP 7: CREATE ENSEMBLE\")\nprint(\"=\" * 60)\n\n# Load all predictions\nprint(\"\\nLoading predictions...\")\n\n# Check if ResNet18 baseline exists, if not skip it\nuse_resnet18 = os.path.exists('data/submission.csv')\n\nif use_resnet18:\n    pred_resnet18 = pd.read_csv('data/submission.csv')\n    print(\"‚úÖ ResNet18 baseline found (82.3%)\")\nelse:\n    print(\"‚ö†Ô∏è  ResNet18 baseline not found, using 3-model ensemble\")\n\npred_resnet50 = pd.read_csv('data/submission_resnet50.csv')\npred_densenet = pd.read_csv('data/submission_densenet121.csv')\npred_mobilenet = pd.read_csv('data/submission_mobilenetv2.csv')\n\nprob_cols = ['normal', 'bacteria', 'virus', 'COVID-19']\n\nif use_resnet18:\n    # 4-Model Ensemble\n    print(\"\\n[1/2] Creating 4-model simple average ensemble...\")\n    ensemble_simple = pred_resnet18.copy()\n    ensemble_simple[prob_cols] = (\n        pred_resnet18[prob_cols].values +\n        pred_resnet50[prob_cols].values +\n        pred_densenet[prob_cols].values +\n        pred_mobilenet[prob_cols].values\n    ) / 4.0\n    \n    predictions = ensemble_simple[prob_cols].values.argmax(axis=1)\n    ensemble_simple[prob_cols] = np.eye(4)[predictions]\n    ensemble_simple.to_csv('submission_ensemble_4way_simple.csv', index=False)\n    print(\"‚úÖ submission_ensemble_4way_simple.csv\")\n    \n    print(\"\\n[2/2] Creating 4-model weighted average ensemble...\")\n    weights = np.array([0.82, 0.85, 0.85, 0.84])\n    weights = weights / weights.sum()\n    print(f\"   Weights: ResNet18={weights[0]:.3f}, ResNet50={weights[1]:.3f}, DenseNet={weights[2]:.3f}, MobileNet={weights[3]:.3f}\")\n    \n    ensemble_weighted = pred_resnet18.copy()\n    ensemble_weighted[prob_cols] = (\n        weights[0] * pred_resnet18[prob_cols].values +\n        weights[1] * pred_resnet50[prob_cols].values +\n        weights[2] * pred_densenet[prob_cols].values +\n        weights[3] * pred_mobilenet[prob_cols].values\n    )\n    \n    predictions = ensemble_weighted[prob_cols].values.argmax(axis=1)\n    ensemble_weighted[prob_cols] = np.eye(4)[predictions]\n    ensemble_weighted.to_csv('submission_ensemble_4way_weighted.csv', index=False)\n    print(\"‚úÖ submission_ensemble_4way_weighted.csv\")\n    \nelse:\n    # 3-Model Ensemble (without ResNet18 baseline)\n    print(\"\\n[1/2] Creating 3-model simple average ensemble...\")\n    ensemble_simple = pred_resnet50.copy()\n    ensemble_simple[prob_cols] = (\n        pred_resnet50[prob_cols].values +\n        pred_densenet[prob_cols].values +\n        pred_mobilenet[prob_cols].values\n    ) / 3.0\n    \n    predictions = ensemble_simple[prob_cols].values.argmax(axis=1)\n    ensemble_simple[prob_cols] = np.eye(4)[predictions]\n    ensemble_simple.to_csv('submission_ensemble_3way_simple.csv', index=False)\n    print(\"‚úÖ submission_ensemble_3way_simple.csv\")\n    \n    print(\"\\n[2/2] Creating 3-model weighted average ensemble...\")\n    weights = np.array([0.85, 0.85, 0.84])\n    weights = weights / weights.sum()\n    print(f\"   Weights: ResNet50={weights[0]:.3f}, DenseNet={weights[1]:.3f}, MobileNet={weights[2]:.3f}\")\n    \n    ensemble_weighted = pred_resnet50.copy()\n    ensemble_weighted[prob_cols] = (\n        weights[0] * pred_resnet50[prob_cols].values +\n        weights[1] * pred_densenet[prob_cols].values +\n        weights[2] * pred_mobilenet[prob_cols].values\n    )\n    \n    predictions = ensemble_weighted[prob_cols].values.argmax(axis=1)\n    ensemble_weighted[prob_cols] = np.eye(4)[predictions]\n    ensemble_weighted.to_csv('submission_ensemble_3way_weighted.csv', index=False)\n    print(\"‚úÖ submission_ensemble_3way_weighted.csv\")\n\n# Show distributions\nprint(\"\\nüìä Prediction distributions:\")\nfor name, df in [('Simple', ensemble_simple), ('Weighted', ensemble_weighted)]:\n    print(f\"\\n{name} ensemble:\")\n    counts = df[prob_cols].sum()\n    for cls, count in counts.items():\n        pct = count / len(df) * 100\n        print(f\"  {cls:12s}: {int(count):4d} ({pct:5.2f}%)\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚úÖ ENSEMBLE COMPLETE!\")\nprint(\"=\" * 60)\nprint(\"\\nüìä EXPECTED SCORES:\")\nif use_resnet18:\n    print(\"   - Simple average (4-model): 87-89%\")\n    print(\"   - Weighted average (4-model): 88-90% üéØ\")\n    print(\"\\nüí° RECOMMENDED: Upload submission_ensemble_4way_weighted.csv\")\nelse:\n    print(\"   - Simple average (3-model): 85-87%\")\n    print(\"   - Weighted average (3-model): 86-88% üéØ\")\n    print(\"\\nüí° RECOMMENDED: Upload submission_ensemble_3way_weighted.csv\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Download Ensemble Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files as colab_files\n\nprint(\"=\" * 60)\nprint(\"STEP 8: DOWNLOAD SUBMISSIONS\")\nprint(\"=\" * 60)\n\n# Check which ensemble files were created\nfiles_to_download = []\nfor f in ['submission_ensemble_4way_simple.csv', 'submission_ensemble_4way_weighted.csv',\n          'submission_ensemble_3way_simple.csv', 'submission_ensemble_3way_weighted.csv']:\n    if os.path.exists(f):\n        files_to_download.append(f)\n\nif not files_to_download:\n    print(\"‚ùå No ensemble files found!\")\nelse:\n    for file in files_to_download:\n        print(f\"\\nDownloading {file}...\")\n        colab_files.download(file)\n        print(\"‚úÖ Downloaded\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"üéâ ALL DONE!\")\nprint(\"=\" * 60)\nprint(\"\\nüìù NEXT STEPS:\")\nprint(\"   1. Go to https://www.kaggle.com/competitions/cxr-multi-label-classification\")\nprint(\"   2. Click 'Submit Predictions'\")\nprint(\"   3. Upload the *_weighted.csv file\")\nprint(\"   4. Expected score: 86-90%\")\nprint(\"\\nüöÄ Improvement from baseline: 82.3% ‚Üí 86-90%\")\nprint(\"=\" * 60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}