{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-Ray Classification - Optimized for 80%+ Score\n",
    "\n",
    "**Strategy:**\n",
    "- Phase 1: Reproduce 80.122% baseline (ResNet18, simple config)\n",
    "- Phase 2: Train additional models (ResNet34, EfficientNetV2-S)\n",
    "- Phase 3: TTA + Ensemble for 85%+ target\n",
    "\n",
    "**Baseline Model Config (80.122%):**\n",
    "- Model: ResNet18 pretrained\n",
    "- Image Size: 224px\n",
    "- Batch Size: 32 (A100) or 12 (local)\n",
    "- Epochs: 8-12\n",
    "- Loss: CrossEntropy + Label Smoothing 0.05\n",
    "- Optimizer: AdamW, LR 0.0003\n",
    "- Scheduler: Cosine with 1 epoch warmup\n",
    "- Weighted Sampler: True\n",
    "- AMP: bfloat16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q numpy pandas scikit-learn matplotlib tqdm pyyaml opencv-python seaborn albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os, sys, yaml, random\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "# Enable TF32 for A100\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "print(f\"TF32 enabled: {torch.backends.cuda.matmul.allow_tf32}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Upload & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Option 1: Clone from GitHub\n",
    "    # !git clone YOUR_REPO_URL\n",
    "    \n",
    "    # Option 2: Upload zip file from Drive\n",
    "    # !unzip \"/content/drive/MyDrive/nycu-CSIC30014-LAB3.zip\" -d /content/\n",
    "    \n",
    "    # Option 3: Manual upload (for testing)\n",
    "    print(\"Please upload your project folder or use one of the options above\")\n",
    "    \n",
    "    # Set working directory\n",
    "    # os.chdir('/content/nycu-CSIC30014-LAB3')\n",
    "else:\n",
    "    # Local path\n",
    "    os.chdir('C:/Users/thc1006/Desktop/114-1/nycu-CSIC30014-LAB3')\n",
    "\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Files: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('data/train_data.csv')\n",
    "val_df = pd.read_csv('data/val_data.csv')\n",
    "test_df = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "print(f\"Train: {len(train_df)} samples\")\n",
    "print(f\"Val: {len(val_df)} samples\")\n",
    "print(f\"Test: {len(test_df)} samples\")\n",
    "\n",
    "# Class distribution\n",
    "classes = ['normal', 'bacteria', 'virus', 'COVID-19']\n",
    "train_dist = {cls: train_df[cls].sum() for cls in classes}\n",
    "print(\"\\nTrain distribution:\")\n",
    "for cls, count in train_dist.items():\n",
    "    print(f\"  {cls:10s}: {count:4d} ({count/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for ax, (name, df) in zip(axes, [('Train', train_df), ('Val', val_df), ('Test', test_df)]):\n",
    "    counts = [df[cls].sum() for cls in classes]\n",
    "    ax.bar(classes, counts)\n",
    "    ax.set_title(f\"{name} Distribution (n={len(df)})\")\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "# Dataset\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = ['normal', 'bacteria', 'virus', 'COVID-19']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.img_dir / row['new_filename']\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Get label (argmax of one-hot)\n",
    "        label = np.argmax([row[cls] for cls in self.classes])\n",
    "        \n",
    "        return img, label, row['new_filename']\n",
    "\n",
    "# Model builder\n",
    "def build_model(name, num_classes=4):\n",
    "    if name == \"resnet18\":\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif name == \"resnet34\":\n",
    "        model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif name == \"efficientnet_v2_s\":\n",
    "        model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {name}\")\n",
    "    return model\n",
    "\n",
    "# Loss function with label smoothing\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, smoothing=0.05, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, logits, targets):\n",
    "        log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "        targets_one_hot = nn.functional.one_hot(targets, self.num_classes).float()\n",
    "        targets_smooth = (1 - self.smoothing) * targets_one_hot + self.smoothing / self.num_classes\n",
    "        loss = -(targets_smooth * log_probs).sum(dim=-1).mean()\n",
    "        return loss\n",
    "\n",
    "# Cosine LR scheduler\n",
    "def cosine_lr(optimizer, base_lr, warmup_steps, total_steps):\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, loader, optimizer, scaler, device, loss_fn, use_amp=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    for imgs, targets, _ in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if use_amp:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                logits = model(imgs)\n",
    "                loss = loss_fn(logits, targets)\n",
    "        else:\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, targets)\n",
    "        \n",
    "        if scaler and use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = logits.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    for imgs, targets, _ in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    return acc, f1, cm\n",
    "\n",
    "print(\"Core functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 1: Baseline Model (80.122% Reproduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Config for baseline (80.122%)\n",
    "BASELINE_CONFIG = {\n",
    "    'model_name': 'resnet18',\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32 if IN_COLAB else 12,  # A100 can handle 32\n",
    "    'epochs': 12 if IN_COLAB else 8,\n",
    "    'lr': 0.0003,\n",
    "    'weight_decay': 0.0001,\n",
    "    'label_smoothing': 0.05,\n",
    "    'warmup_epochs': 1,\n",
    "    'use_weighted_sampler': True,\n",
    "    'num_workers': 4,\n",
    "}\n",
    "\n",
    "print(\"Baseline Configuration:\")\n",
    "for k, v in BASELINE_CONFIG.items():\n",
    "    print(f\"  {k:25s}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transforms (BASELINE - keep it simple!)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((BASELINE_CONFIG['img_size'], BASELINE_CONFIG['img_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((BASELINE_CONFIG['img_size'], BASELINE_CONFIG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = XRayDataset('data/train_data.csv', 'train_images', train_transform)\n",
    "val_dataset = XRayDataset('data/val_data.csv', 'val_images', val_transform)\n",
    "\n",
    "# Weighted sampler for class imbalance\n",
    "if BASELINE_CONFIG['use_weighted_sampler']:\n",
    "    train_df = pd.read_csv('data/train_data.csv')\n",
    "    classes = ['normal', 'bacteria', 'virus', 'COVID-19']\n",
    "    train_labels = np.argmax(train_df[classes].values, axis=1)\n",
    "    class_counts = np.bincount(train_labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = class_weights[train_labels]\n",
    "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "    shuffle = False\n",
    "    print(f\"Using WeightedRandomSampler | Class weights: {class_weights}\")\n",
    "else:\n",
    "    sampler = None\n",
    "    shuffle = True\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BASELINE_CONFIG['batch_size'],\n",
    "    sampler=sampler,\n",
    "    shuffle=shuffle if sampler is None else False,\n",
    "    num_workers=BASELINE_CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BASELINE_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=BASELINE_CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_baseline = build_model(BASELINE_CONFIG['model_name']).to(device)\n",
    "model_baseline = model_baseline.to(memory_format=torch.channels_last)  # Optimize for A100\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    model_baseline.parameters(),\n",
    "    lr=BASELINE_CONFIG['lr'],\n",
    "    weight_decay=BASELINE_CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = BASELINE_CONFIG['epochs'] * steps_per_epoch\n",
    "warmup_steps = BASELINE_CONFIG['warmup_epochs'] * steps_per_epoch\n",
    "\n",
    "scheduler = cosine_lr(optimizer, BASELINE_CONFIG['lr'], warmup_steps, total_steps)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = LabelSmoothingCE(smoothing=BASELINE_CONFIG['label_smoothing'])\n",
    "\n",
    "# AMP scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model_baseline.parameters()) / 1e6:.2f}M\")\n",
    "print(f\"Total steps: {total_steps} | Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Baseline Training (Target: 80%+ F1)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "best_val_f1 = 0\n",
    "history = {'train_loss': [], 'train_acc': [], 'train_f1': [], 'val_acc': [], 'val_f1': []}\n",
    "\n",
    "for epoch in range(BASELINE_CONFIG['epochs']):\n",
    "    print(f\"\\nEpoch {epoch+1}/{BASELINE_CONFIG['epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model_baseline, train_loader, optimizer, scaler, device, loss_fn, use_amp=True\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_acc, val_f1, val_cm = evaluate(model_baseline, val_loader, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    for _ in range(steps_per_epoch):\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Log\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"  Val   Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model_baseline.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_f1': val_f1,\n",
    "            'config': BASELINE_CONFIG\n",
    "        }, 'baseline_best.pt')\n",
    "        print(f\"  -> Saved best model (Val F1: {val_f1:.4f})\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Baseline Training Complete!\")\n",
    "print(f\"Best Val F1: {best_val_f1:.4f} (Target: 0.80+)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1].plot(history['train_f1'], label='Train F1', marker='o')\n",
    "axes[1].plot(history['val_f1'], label='Val F1', marker='s')\n",
    "axes[1].axhline(y=0.80, color='r', linestyle='--', label='Target (80%)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Macro F1 Score')\n",
    "axes[1].set_title(f'F1 Score (Best Val: {best_val_f1:.4f})')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('baseline_training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix\n",
    "classes = ['Normal', 'Bacteria', 'Virus', 'COVID-19']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title(f'Validation Confusion Matrix (F1: {val_f1:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('baseline_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Baseline Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best baseline model\n",
    "checkpoint = torch.load('baseline_best.pt')\n",
    "model_baseline.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best baseline model (Val F1: {checkpoint['val_f1']:.4f})\")\n",
    "\n",
    "# Prepare test data\n",
    "test_dataset = XRayDataset('data/test_data.csv', 'test_images', val_transform)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BASELINE_CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=BASELINE_CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_filenames = []\n",
    "    \n",
    "    for imgs, _, filenames in tqdm(loader, desc=\"Predicting\"):\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_filenames.extend(filenames)\n",
    "    \n",
    "    all_probs = np.vstack(all_probs)\n",
    "    return all_probs, all_filenames\n",
    "\n",
    "baseline_probs, filenames = predict(model_baseline, test_loader, device)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'new_filename': filenames,\n",
    "    'normal': baseline_probs[:, 0],\n",
    "    'bacteria': baseline_probs[:, 1],\n",
    "    'virus': baseline_probs[:, 2],\n",
    "    'COVID-19': baseline_probs[:, 3]\n",
    "})\n",
    "\n",
    "# Convert to one-hot (hardmax)\n",
    "pred_classes = baseline_probs.argmax(axis=1)\n",
    "one_hot = np.eye(4)[pred_classes]\n",
    "submission_df[['normal', 'bacteria', 'virus', 'COVID-19']] = one_hot\n",
    "\n",
    "# Save\n",
    "submission_df.to_csv('submission_baseline.csv', index=False)\n",
    "print(f\"\\nBaseline submission saved to: submission_baseline.csv\")\n",
    "print(f\"Predictions: {len(submission_df)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for i, cls in enumerate(['normal', 'bacteria', 'virus', 'COVID-19']):\n",
    "    print(f\"  {cls:10s}: {(pred_classes == i).sum():4d} ({(pred_classes == i).sum()/len(pred_classes)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 2: Additional Models (Optional - for Ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train additional models for ensemble\n",
    "# This is optional - only if you have time and want to push beyond 80%\n",
    "\n",
    "ADDITIONAL_MODELS = [\n",
    "    {'name': 'resnet34', 'img_size': 256, 'epochs': 10},\n",
    "    {'name': 'efficientnet_v2_s', 'img_size': 288, 'epochs': 12},\n",
    "]\n",
    "\n",
    "# Uncomment to train additional models\n",
    "# trained_models = []\n",
    "# for config in ADDITIONAL_MODELS:\n",
    "#     print(f\"\\nTraining {config['name']}...\")\n",
    "#     # Train model (similar to baseline)\n",
    "#     # Save model and predictions\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Phase 3: Test-Time Augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA - Multiple augmented predictions averaged\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Applying Test-Time Augmentation (TTA)\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Define TTA transforms\n",
    "tta_transforms = [\n",
    "    # Original\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((BASELINE_CONFIG['img_size'], BASELINE_CONFIG['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Horizontal flip\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((BASELINE_CONFIG['img_size'], BASELINE_CONFIG['img_size'])),\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Slight rotation +5\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((BASELINE_CONFIG['img_size'], BASELINE_CONFIG['img_size'])),\n",
    "        transforms.RandomRotation(degrees=(5, 5)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    # Slight rotation -5\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((BASELINE_CONFIG['img_size'], BASELINE_CONFIG['img_size'])),\n",
    "        transforms.RandomRotation(degrees=(-5, -5)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "]\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_tta(model, test_df, img_dir, transforms_list, device, batch_size=32):\n",
    "    model.eval()\n",
    "    all_probs_tta = []\n",
    "    \n",
    "    for i, transform in enumerate(transforms_list):\n",
    "        print(f\"TTA {i+1}/{len(transforms_list)}...\", end=' ')\n",
    "        dataset = XRayDataset(test_df, img_dir, transform)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "        \n",
    "        batch_probs = []\n",
    "        for imgs, _, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            logits = model(imgs)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            batch_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        all_probs_tta.append(np.vstack(batch_probs))\n",
    "        print(f\"Done\")\n",
    "    \n",
    "    # Average all TTA predictions\n",
    "    avg_probs = np.mean(all_probs_tta, axis=0)\n",
    "    return avg_probs\n",
    "\n",
    "# Apply TTA\n",
    "tta_probs = predict_tta(\n",
    "    model_baseline, \n",
    "    'data/test_data.csv',\n",
    "    'test_images',\n",
    "    tta_transforms,\n",
    "    device,\n",
    "    batch_size=BASELINE_CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "# Create TTA submission\n",
    "tta_pred_classes = tta_probs.argmax(axis=1)\n",
    "tta_one_hot = np.eye(4)[tta_pred_classes]\n",
    "\n",
    "submission_tta = pd.DataFrame({\n",
    "    'new_filename': filenames,\n",
    "    'normal': tta_one_hot[:, 0],\n",
    "    'bacteria': tta_one_hot[:, 1],\n",
    "    'virus': tta_one_hot[:, 2],\n",
    "    'COVID-19': tta_one_hot[:, 3]\n",
    "})\n",
    "\n",
    "submission_tta.to_csv('submission_baseline_tta.csv', index=False)\n",
    "print(f\"\\nTTA submission saved to: submission_baseline_tta.csv\")\n",
    "print(f\"\\nTTA Class distribution:\")\n",
    "for i, cls in enumerate(['normal', 'bacteria', 'virus', 'COVID-19']):\n",
    "    print(f\"  {cls:10s}: {(tta_pred_classes == i).sum():4d} ({(tta_pred_classes == i).sum()/len(tta_pred_classes)*100:.1f}%)\")\n",
    "\n",
    "# Compare with baseline\n",
    "differences = (pred_classes != tta_pred_classes).sum()\n",
    "print(f\"\\nDifferences from baseline: {differences}/{len(pred_classes)} ({differences/len(pred_classes)*100:.1f}%)\")\n",
    "print(f\"Expected improvement: +0.5% to +1.5% F1 score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"Downloading submission files...\")\n",
    "    files.download('submission_baseline.csv')\n",
    "    files.download('submission_baseline_tta.csv')\n",
    "    files.download('baseline_best.pt')\n",
    "    files.download('baseline_training_history.png')\n",
    "    files.download('baseline_confusion_matrix.png')\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Files saved locally:\")\n",
    "    print(\"  - submission_baseline.csv\")\n",
    "    print(\"  - submission_baseline_tta.csv\")\n",
    "    print(\"  - baseline_best.pt\")\n",
    "    print(\"  - baseline_training_history.png\")\n",
    "    print(\"  - baseline_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" \"*25 + \"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n1. BASELINE MODEL (ResNet18):\")\n",
    "print(f\"   - Val F1 Score: {best_val_f1:.4f}\")\n",
    "print(f\"   - Target: 0.80+ (80.122% baseline)\")\n",
    "print(f\"   - Status: {'PASSED' if best_val_f1 >= 0.80 else 'NEEDS MORE TRAINING'}\")\n",
    "print(f\"\\n2. SUBMISSIONS GENERATED:\")\n",
    "print(f\"   - submission_baseline.csv (standard prediction)\")\n",
    "print(f\"   - submission_baseline_tta.csv (with TTA, expected +0.5-1.5% improvement)\")\n",
    "print(f\"\\n3. RECOMMENDED SUBMISSION:\")\n",
    "print(f\"   - Use: submission_baseline_tta.csv\")\n",
    "print(f\"   - Expected Public Score: 80-82%\")\n",
    "print(f\"\\n4. FURTHER IMPROVEMENTS (Optional):\")\n",
    "print(f\"   - Train ResNet34 or EfficientNetV2-S (Section 7)\")\n",
    "print(f\"   - Ensemble multiple models (soft voting)\")\n",
    "print(f\"   - Longer training (20+ epochs with early stopping)\")\n",
    "print(f\"   - Advanced data augmentation (CLAHE, MixUp)\")\n",
    "print(f\"   - Pseudo-labeling with test set\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nGood luck with your submission!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
