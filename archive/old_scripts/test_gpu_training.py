#!/usr/bin/env python3
"""å¿«é€Ÿæ¸¬è©¦ GPU è¨“ç·´ç’°å¢ƒ"""
import torch
import torch.nn as nn
from pathlib import Path

print("=" * 80)
print("GPU è¨“ç·´ç’°å¢ƒæ¸¬è©¦")
print("=" * 80)

# 1. PyTorch å’Œ CUDA
print(f"\nâœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"âœ… CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… GPU è£ç½®: {torch.cuda.get_device_name(0)}")
    print(f"âœ… GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# 2. æ¸¬è©¦ GPU è¨ˆç®—
print(f"\nâš¡ æ¸¬è©¦ GPU è¨ˆç®—...")
x = torch.randn(1000, 1000).cuda()
y = torch.randn(1000, 1000).cuda()
z = x @ y
print(f"âœ… GPU çŸ©é™£ä¹˜æ³•æ¸¬è©¦é€šé: {z.shape}")

# 3. æ¸¬è©¦æ··åˆç²¾åº¦
print(f"\nâš¡ æ¸¬è©¦æ··åˆç²¾åº¦è¨“ç·´...")
model = nn.Linear(1000, 100).cuda()
input_data = torch.randn(32, 1000).cuda()
with torch.cuda.amp.autocast():
    output = model(input_data)
print(f"âœ… æ··åˆç²¾åº¦æ¸¬è©¦é€šé: {output.dtype}")

# 4. æª¢æŸ¥å½±åƒç›®éŒ„
print(f"\nğŸ“ æª¢æŸ¥è³‡æ–™ç›®éŒ„...")
train_dir = Path("train_images")
val_dir = Path("val_images")
test_dir = Path("test_images")

train_count = len(list(train_dir.glob("*")))
val_count = len(list(val_dir.glob("*")))
test_count = len(list(test_dir.glob("*")))

print(f"âœ… è¨“ç·´å½±åƒ: {train_count} å¼µ")
print(f"âœ… é©—è­‰å½±åƒ: {val_count} å¼µ")
print(f"âœ… æ¸¬è©¦å½±åƒ: {test_count} å¼µ")

# 5. æª¢æŸ¥ CSV
print(f"\nğŸ“‹ æª¢æŸ¥ CSV æª”æ¡ˆ...")
csv_files = ["data/train_data.csv", "data/val_data.csv", "data/test_data.csv"]
for csv_file in csv_files:
    if Path(csv_file).exists():
        print(f"âœ… {csv_file}")
    else:
        print(f"âŒ {csv_file} ä¸å­˜åœ¨")

print("\n" + "=" * 80)
print("âœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼æº–å‚™é–‹å§‹è¨“ç·´ï¼")
print("=" * 80)
