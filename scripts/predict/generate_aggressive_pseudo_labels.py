#!/usr/bin/env python3
"""
ç”Ÿæˆæ¿€é€²å½æ¨™ç±¤ - æ–¹æ¡ˆ B
ä½¿ç”¨ç•¶å‰æœ€ä½³æ¨¡å‹é›†æˆï¼Œé™ä½é–¾å€¼è‡³ 0.72ï¼Œç”Ÿæˆ ~2000+ å½æ¨™ç±¤
"""

import torch
import torch.nn as nn
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
import timm

class TestDataset(Dataset):
    """æ¸¬è©¦é›†æ•¸æ“šé›†"""
    def __init__(self, image_dir, transform=None):
        self.image_dir = Path(image_dir)
        self.image_files = sorted(list(self.image_dir.glob('*.*')))
        self.transform = transform

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, img_path.name

def load_champion_model(checkpoint_path, model_name, num_classes=4, img_size=384):
    """è¼‰å…¥ Champion æ¨¡å‹"""
    # å‰µå»ºæ¨¡å‹
    model = timm.create_model(
        model_name,
        pretrained=False,
        num_classes=num_classes,
        drop_rate=0.3,
        drop_path_rate=0.2
    )

    # è¼‰å…¥æ¬Šé‡
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)

    model = model.cuda()
    model.eval()

    return model

def predict_with_ensemble(models, dataloader, tta=True):
    """
    ä½¿ç”¨é›†æˆæ¨¡å‹é€²è¡Œé æ¸¬

    Args:
        models: List of (model, weight) tuples
        dataloader: æ•¸æ“šåŠ è¼‰å™¨
        tta: æ˜¯å¦ä½¿ç”¨æ¸¬è©¦æ™‚å¢å¼·
    """
    all_predictions = []
    all_filenames = []

    with torch.no_grad():
        for images, filenames in tqdm(dataloader, desc="Generating predictions"):
            images = images.cuda()

            # é›†æˆæ‰€æœ‰æ¨¡å‹çš„é æ¸¬
            ensemble_logits = None
            total_weight = 0

            for model, weight in models:
                # åŸºç¤é æ¸¬
                logits = model(images)

                # TTA: æ°´å¹³ç¿»è½‰
                if tta:
                    logits_flip = model(torch.flip(images, dims=[3]))
                    logits = (logits + logits_flip) / 2

                # åŠ æ¬Š
                if ensemble_logits is None:
                    ensemble_logits = logits * weight
                else:
                    ensemble_logits += logits * weight
                total_weight += weight

            # æ­¸ä¸€åŒ–æ¬Šé‡
            ensemble_logits = ensemble_logits / total_weight

            # Softmax ç²å¾—æ¦‚ç‡
            probs = torch.softmax(ensemble_logits, dim=1)

            all_predictions.append(probs.cpu().numpy())
            all_filenames.extend(filenames)

    all_predictions = np.concatenate(all_predictions, axis=0)
    return all_predictions, all_filenames

def generate_tiered_pseudo_labels(predictions, filenames, test_dir):
    """
    ç”Ÿæˆä¸‰å±¤å½æ¨™ç±¤

    Tier 1 (0.85+): é«˜ç½®ä¿¡åº¦ï¼Œæ¬Šé‡ 1.0
    Tier 2 (0.75-0.85): ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œæ¬Šé‡ 0.6
    Tier 3 (0.72-0.75): ä½ç½®ä¿¡åº¦ï¼Œæ¬Šé‡ 0.3
    """

    max_probs = predictions.max(axis=1)
    pred_labels = predictions.argmax(axis=1)

    # ä¸‰å±¤åˆ†å±¤
    tier1_mask = max_probs >= 0.85
    tier2_mask = (max_probs >= 0.75) & (max_probs < 0.85)
    tier3_mask = (max_probs >= 0.72) & (max_probs < 0.75)

    results = {
        'tier1': {'filenames': [], 'labels': [], 'confidences': [], 'weight': 1.0},
        'tier2': {'filenames': [], 'labels': [], 'confidences': [], 'weight': 0.6},
        'tier3': {'filenames': [], 'labels': [], 'confidences': [], 'weight': 0.3}
    }

    class_names = ['normal', 'bacteria', 'virus', 'COVID-19']

    # Tier 1
    for i in np.where(tier1_mask)[0]:
        results['tier1']['filenames'].append(filenames[i])
        results['tier1']['labels'].append(class_names[pred_labels[i]])
        results['tier1']['confidences'].append(max_probs[i])

    # Tier 2
    for i in np.where(tier2_mask)[0]:
        results['tier2']['filenames'].append(filenames[i])
        results['tier2']['labels'].append(class_names[pred_labels[i]])
        results['tier2']['confidences'].append(max_probs[i])

    # Tier 3
    for i in np.where(tier3_mask)[0]:
        results['tier3']['filenames'].append(filenames[i])
        results['tier3']['labels'].append(class_names[pred_labels[i]])
        results['tier3']['confidences'].append(max_probs[i])

    return results

def save_pseudo_labels(results, output_dir):
    """ä¿å­˜å½æ¨™ç±¤åˆ° CSV"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("\n" + "="*60)
    print("ğŸ“Š æ¿€é€²å½æ¨™ç±¤ç”Ÿæˆå ±å‘Š")
    print("="*60)

    total_samples = 0

    for tier_name, data in results.items():
        tier_num = tier_name[-1]
        n_samples = len(data['filenames'])
        total_samples += n_samples

        if n_samples == 0:
            continue

        # å‰µå»º DataFrame
        df = pd.DataFrame({
            'filename': data['filenames'],
            'label': data['labels'],
            'confidence': data['confidences'],
            'weight': data['weight']
        })

        # ä¿å­˜
        output_path = output_dir / f'pseudo_labels_{tier_name}.csv'
        df.to_csv(output_path, index=False)

        # çµ±è¨ˆ
        print(f"\n{tier_name.upper()} (confidence >= {0.85 if tier_num=='1' else 0.75 if tier_num=='2' else 0.72}):")
        print(f"  æ¨£æœ¬æ•¸: {n_samples}")
        print(f"  æ¬Šé‡: {data['weight']}")
        print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(data['confidences']):.4f}")
        print(f"  é¡åˆ¥åˆ†å¸ƒ:")
        for label in ['normal', 'bacteria', 'virus', 'COVID-19']:
            count = df['label'].value_counts().get(label, 0)
            print(f"    {label}: {count} ({count/n_samples*100:.1f}%)")
        print(f"  æ–‡ä»¶: {output_path}")

    # åˆä½µä¿å­˜
    all_data = []
    for tier_name, data in results.items():
        if len(data['filenames']) > 0:
            df = pd.DataFrame({
                'filename': data['filenames'],
                'label': data['labels'],
                'confidence': data['confidences'],
                'weight': data['weight'],
                'tier': tier_name
            })
            all_data.append(df)

    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        output_path = output_dir / 'pseudo_labels_combined.csv'
        combined_df.to_csv(output_path, index=False)

        print(f"\n{'='*60}")
        print(f"ç¸½è¨ˆ: {total_samples} å€‹å½æ¨™ç±¤")
        print(f"åˆä½µæ–‡ä»¶: {output_path}")
        print(f"{'='*60}\n")

        # èˆ‡åŸå§‹å½æ¨™ç±¤å°æ¯”
        print("ğŸ“ˆ ç›¸æ¯”åŸå§‹å½æ¨™ç±¤:")
        print(f"  åŸå§‹ (threshold=0.80): 1065 å€‹")
        print(f"  æ¿€é€² (threshold=0.72): {total_samples} å€‹")
        print(f"  å¢åŠ : +{total_samples - 1065} å€‹ (+{(total_samples-1065)/1065*100:.1f}%)")

def main():
    """ä¸»å‡½æ•¸"""

    # é…ç½®
    TEST_DIR = Path('data/test')
    OUTPUT_DIR = Path('data/pseudo_labels_aggressive')
    IMG_SIZE = 384
    BATCH_SIZE = 32

    print("ğŸš€ æ¿€é€²å½æ¨™ç±¤ç”Ÿæˆ - æ–¹æ¡ˆ B")
    print("="*60)
    print(f"æ¸¬è©¦é›†ç›®éŒ„: {TEST_DIR}")
    print(f"è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}")
    print(f"åœ–ç‰‡å¤§å°: {IMG_SIZE}")
    print(f"é–¾å€¼è¨­å®š:")
    print(f"  Tier 1: >= 0.85 (æ¬Šé‡ 1.0)")
    print(f"  Tier 2: 0.75-0.85 (æ¬Šé‡ 0.6)")
    print(f"  Tier 3: 0.72-0.75 (æ¬Šé‡ 0.3)")
    print("="*60)

    # æ•¸æ“šè½‰æ›
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                           std=[0.229, 0.224, 0.225])
    ])

    # å‰µå»ºæ•¸æ“šé›†å’ŒåŠ è¼‰å™¨
    test_dataset = TestDataset(TEST_DIR, transform=transform)
    test_loader = DataLoader(
        test_dataset,
        batch_size=BATCH_SIZE,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )

    print(f"\nğŸ“ æ¸¬è©¦é›†æ¨£æœ¬æ•¸: {len(test_dataset)}")

    # è¼‰å…¥æœ€ä½³çš„ Champion æ¨¡å‹
    print("\nğŸ”§ è¼‰å…¥ Champion æ¨¡å‹...")

    models = []
    model_configs = [
        # ConvNeXt-Large (3 folds, weight 1.0 each)
        ('outputs/champion_convnext_large/fold0/best.pt', 'convnext_large', 1.0),
        ('outputs/champion_convnext_large/fold1/best.pt', 'convnext_large', 1.0),
        ('outputs/champion_convnext_large/fold2/best.pt', 'convnext_large', 1.0),
        # ViT-Large (2 folds, weight 1.3 each)
        ('outputs/champion_vit_large/fold0/best.pt', 'vit_large_patch16_384', 1.3),
        ('outputs/champion_vit_large/fold1/best.pt', 'vit_large_patch16_384', 1.3),
        # BEiT-Large (2 folds, weight 1.3 each)
        ('outputs/champion_beit_large/fold0/best.pt', 'beit_large_patch16_384', 1.3),
        ('outputs/champion_beit_large/fold1/best.pt', 'beit_large_patch16_384', 1.3),
    ]

    for checkpoint_path, model_name, weight in model_configs:
        checkpoint_path = Path(checkpoint_path)
        if checkpoint_path.exists():
            try:
                print(f"  âœ“ {model_name} from {checkpoint_path.parent.name} (weight={weight})")
                model = load_champion_model(checkpoint_path, model_name, num_classes=4, img_size=IMG_SIZE)
                models.append((model, weight))
            except Exception as e:
                print(f"  âœ— Failed to load {checkpoint_path}: {e}")
        else:
            print(f"  âœ— Not found: {checkpoint_path}")

    print(f"\nâœ… æˆåŠŸè¼‰å…¥ {len(models)} å€‹æ¨¡å‹")

    # ç”Ÿæˆé æ¸¬
    print("\nğŸ”® ç”Ÿæˆé›†æˆé æ¸¬ (with TTA)...")
    predictions, filenames = predict_with_ensemble(models, test_loader, tta=True)

    # ç”Ÿæˆä¸‰å±¤å½æ¨™ç±¤
    print("\nğŸ“ ç”Ÿæˆä¸‰å±¤å½æ¨™ç±¤...")
    results = generate_tiered_pseudo_labels(predictions, filenames, TEST_DIR)

    # ä¿å­˜çµæœ
    save_pseudo_labels(results, OUTPUT_DIR)

    print("\nâœ… æ¿€é€²å½æ¨™ç±¤ç”Ÿæˆå®Œæˆï¼")
    print(f"ä¸‹ä¸€æ­¥: ä½¿ç”¨é€™äº›å½æ¨™ç±¤é‡æ–°è¨“ç·´ Top 3 æ¨¡å‹")

if __name__ == '__main__':
    main()
